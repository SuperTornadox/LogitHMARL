\documentclass[journal]{IEEEtran}

% ---------- 基础宏包 ----------
\usepackage[utf8]{inputenc}      % 让 pdfLaTeX 正确处理 UTF-8
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{color,array,xspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{parskip}            % 段落前后空白更友好
\makeatletter
\@ifundefined{labelindent}{}{\let\labelindent\relax}
\makeatother
\usepackage{enumitem}           % 自定义 enumerate 样式
\usepackage[numbers]{natbib}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[colorlinks,urlcolor=blue,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage[capitalise]{cleveref}

\DeclareUnicodeCharacter{2009}{\,}         % U+2009 细空格 → LaTeX 细空格
\DeclareUnicodeCharacter{2192}{$\rightarrow$} % U+2192 → 数学右箭头

% ---------- 算法环境（任选其一；此处保留 algorithm2e） ----------
\usepackage[linesnumberedhidden]{algorithm2e}
%\usepackage{algorithm}
%\usepackage{algpseudocode}


\begin{document}
% ---------- 题目与作者 ----------
\title{Nested-Logit Hierarchical MARL for Real-Time Task Allocation in Robotic Warehouses}
\author{%
  Xuchen He$^{1}$,
  Vijay K.~Madisetti$^{2}$,~\IEEEmembership{Fellow,~IEEE}%
  \thanks{$^{1}$Xuchen He is with the School of Computing,
          Georgia Institute of Technology, Atlanta, GA 30332 USA
          (e-mail: xhe402@gatech.edu).}%
  \thanks{$^{2}$V. K. Madisetti is with the School of Cybersecurity \& Priv,
          Georgia Institute of Technology, Atlanta, GA 30332 USA
          (e-mail: vkm@gatech.edu).}%
}
\maketitle

\section{Introduction}
\IEEEPARstart{G}{lobal} e‑commerce growth and ever‑shorter delivery promises have transformed large fulfilment centres into highly dynamic cyber–physical systems populated by hundreds of autonomous mobile robots (AMRs) and human pickers.%
A central bottleneck in such environments is \emph{real‑time task allocation}: deciding which agent should execute which job (pick, move, recharge, etc.) so that throughput is maximised and congestion is avoided.%
Formally, the problem couples (i) a stochastic arrival process of tasks, (ii) a spatially constrained routing domain, and (iii) resource conflicts such as narrow aisles and shared chargers.

\vspace{2pt}
\textbf{Limitations of existing approaches.} %
Conventional rule‑based dispatchers or mixed‑integer programming formulations make strong assumptions about complete knowledge and often re‑optimise only at coarse time scales \citep{ratliff1983sshape,de_koster1997optimal}.%
These methods struggle whenever tasks arrive continuously and system state changes faster than the optimiser can re‑solve.%
More recent \emph{multi‑agent reinforcement learning} (MARL) removes handcrafted heuristics but suffers from the curse of dimensionality: flat MARL must explore a joint action space that grows exponentially with the number of agents and tasks \citep{alam2024dqn}.%
Hierarchical MARL (HMARL) alleviates this by introducing a \emph{manager–worker} structure; however, most managers rely on a categorical policy that implicitly assumes \emph{independence of irrelevant alternatives (IIA)}.%
When a high‑priority task appears in one zone, this assumption causes the manager to re‑weight \emph{all} alternatives—relevant or not—thereby increasing the risk of congestion and idle resources elsewhere.

\vspace{2pt}
\textbf{Motivating idea.} %
To relax the IIA assumption while keeping the hierarchical decomposition, we consider equipping the high‑level manager with a \emph{Nested Logit} (NL) choice mechanism.%
An NL structure first selects a \textit{nest}—for example, all pick tasks in Zone A or all charging jobs—then chooses a concrete task within that nest.%
This two‑stage view preserves correlated alternatives within a nest and treats tasks in different nests as largely independent, matching the spatial and functional groupings observed in real warehouses.%
Designing and learning such an NL‑HMARL architecture raises several open questions: how to define nests from raw warehouse state, how to integrate NL choice probabilities into reinforcement‑learning updates, and how to keep the overall decision loop fast enough for real‑time control.

\vspace{2pt}
\textbf{Scope of this paper.} %
The remainder of this work focuses on formulating the NL‑HMARL framework, outlining its learning algorithm, and discussing how it interfaces with low‑level motion controllers.%
Empirical evaluation and quantitative comparisons are ongoing and will be presented in future revisions; this manuscript restricts itself to problem definition, modelling choices, and methodological details.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ==========================================================
\section{Background and Related Research}
% ==========================================================

Warehouse order-picking has become a showcase domain for multi-agent decision-making: dozens of autonomous mobile robots (AMRs) and human pickers must cooperate in narrow aisles while new orders arrive continuously.  The key challenge is \emph{real-time task allocation}—deciding \textit{who} should execute \textit{which} job so that throughput remains high and congestion is avoided.  Over the past four decades, researchers have offered solutions that range from handcrafted routing rules to learning-based dispatchers.  Table \ref{tab:comparison} gives a side-by-side comparison; the text below distils the main lines of work and their limitations.

\subsection{Hand-crafted Routing Heuristics}
Early warehouse studies proposed geometric heuristics such as \textbf{S-Shape}, \textbf{Return}, and \textbf{Largest-Gap} policies \citep{ratliff1983sshape,de_koster1997optimal}.  
These methods chart deterministic picker paths—e.g.\ entering an aisle from one end and exiting the other—to guarantee full coverage with minimal computation.  
\vspace{-2pt}
\begin{itemize}[leftmargin=1.5em]
  \item[] \textit{Pros:} millisecond execution, intuitive for practitioners.  
  \item[] \textit{Cons:} assume tasks are independent; add extra walking when picks are sparse or aisles are dead-ends.
\end{itemize}

\subsection{Exact and Approximate OR Formulations}
To move beyond single-picker rules, researchers turned to precise optimisation.  
Exact \emph{TSP variants} find the absolute shortest tour but scale exponentially with pick points.  
Approximate dynamic-programming schemes—e.g.\ partition-optimisation DP or accelerated SKU classification—strike a trade-off between solution quality and run-time \citep{saylam2022partition,rao2023accelerated}.  
However, most OR models must be re-solved when new orders arrive, limiting real-time usability in large fulfilment centres.

\subsection{Learning-based Single-Agent Methods}
With the advent of deep RL, \textbf{DQN-style} agents have been trained to plan one step at a time on grid abstractions \citep{alam2024dqn}.  
Such agents respond quickly but face an exploding joint action space when many robots act simultaneously.  
Fine-grained policies also risk myopic oscillations because they lack a global task-assignment view.

\subsection{Hierarchical and Federated MARL}
Hierarchical MARL (HMARL) splits the decision stack: a \emph{manager} allocates tasks, and \emph{worker} policies execute motions \citep{krnjaic2023hierarchical}.  
Federated MARL extends this idea across multiple warehouses while keeping data local \citep{ho2024federated}.  
Although sample-efficient, almost all high-level managers rely on a categorical soft-max choice, which embeds the \emph{independence-of-irrelevant-alternatives} (IIA) assumption—over-penalising unrelated options whenever one task’s utility changes.

\subsection{Modelling Correlated Tasks}
Operations-research literature models correlated choices with \textbf{Nested Logit} (NL) structures that group similar alternatives into “nests’’ \citep{train2009discrete}.  
In warehouse contexts, NL has been applied mainly to static storage design; its integration into online MARL managers—especially with learnable nest dissimilarity parameters—remains unexplored.

\subsection{Research Gap and Outlook}
The survey above exposes three open issues:  
(i)~real-time scalability once task arrivals grow dense,  
(ii)~explicit modelling of correlation among tasks that share space or resources, and  
(iii)~transparent, robust high-level decisions beyond a black-box soft-max.  
The remainder of this paper proposes to embed an NL layer into the HMARL manager, yielding a two-stage \textit{choose-nest → choose-task} policy that learns both utility weights and nest dissimilarities while retaining end-to-end reinforcement-learning updates.

\begin{table*}[t!]
  \centering
  \caption{Comprehensive comparison of routing and learning methods for warehouse order-picking task allocation}
  \label{tab:comparison}
  \renewcommand{\arraystretch}{1.25}

  % -------------- 1 ⟶ \resizebox ---------------
  \resizebox{\textwidth}{!}{%
  % -------------- 2 ⟶ tabular ------------------
  \begin{tabular}{p{2.7cm}p{4.9cm}p{3.2cm}p{4.8cm}p{3.8cm}p{4.2cm}p{2.8cm}p{3.2cm}}
    \toprule
      \textbf{Method} & \textbf{Focus} & \textbf{Reference} & \textbf{Approach} &
      \textbf{Key Benefit} & \textbf{Limitations} &
      \textbf{Computational Complexity} & \textbf{Dynamic Adjustment Ability}\\
    \midrule
      S-Shape Routing Policy &
      Systematically traverses aisles with pick tasks using a predefined S-shaped (or Z-shaped) path to ensure every pick location is covered. &
      Ratliff and Rosenthal (1983). &
      Picker enters an aisle at one end, picks along the way, exits at the other end (for through aisles), then moves to the next aisle in S-shape order. &
      Simple, intuitive, ensures full aisle coverage; most efficient when picks are densely distributed. &
      Adds unnecessary walking if an aisle has few picks (especially near its entrance) or is a dead-end. &
      Low -- $O(P\log P)$ & Low \\[2pt]

      Return Routing Policy &
      Picks all items in an aisle by entering and exiting from the same end. &
      Widely discussed heuristic; no specific origin. &
      Picker enters an aisle, walks to the furthest pick, then returns and leaves from the same end. &
      Ideal for dead-end aisles or when picks cluster near the entrance of long aisles, avoiding full traversal. &
      Less efficient when picks are spread throughout the aisle or when aisles are bidirectional, because it always includes a round-trip. &
      Low -- $O(P\log P)$ & Low \\[2pt]

      Optimal Routing Procedure &
      Finds the absolutely shortest path or minimum time to visit all order items. &
      De Koster \& Van der Poort (1997). &
      Models the problem as a TSP variant or uses dynamic programming, considering all pick points and constraints. &
      Guarantees the shortest pick path, minimising travel distance. &
      Very high computation cost for many pick points or complex layouts, hindering real-time use. &
      Very High -- $O(n^{2}\!\cdot\!2^{n})$ & Very Low \\[2pt]

      Composite Routing Policy &
      Balances path length and computation by combining elements of multiple simple heuristics. &
      De Koster \& Van der Poort (1997). &
      Selects and applies different simple rules (e.g., S-shape within aisles, Largest-Gap between aisles) based on order or zone features. &
      Often outperforms any single heuristic without the extreme cost of optimal algorithms. &
      Performance depends heavily on the chosen rule mix and may need careful tuning. &
      Medium -- $O(H\!\cdot\!P\log P)$ & Medium \\[2pt]

      Largest Gap Routing Policy &
      Minimises walking by allowing bidirectional traversal and exiting the aisle at the end closest to the next pick. &
      De Koster \& Van der Poort (1997). &
      Picker walks to the furthest pick, then exits at the nearer aisle end. &
      More efficient than one-way traversals when aisles are open at both ends and picks are dispersed. &
      Requires both aisle ends open; performance degrades when picks cluster at one end; less intuitive. &
      Low -- $O(P\log P)$ & Medium \\[2pt]

      AGV Cooperative Extension Algorithm &
      Dynamic order assignment and battery management for mixed human–AGV teams. &
      Dehghan et al.\ (2023) &
      Approximate dynamic programming with neural roll-out evaluation, recalculated in a rolling horizon. &
      Improves throughput while balancing battery endurance; out-performs greedy heuristics. &
      Needs an accurate energy model; high training \& simulation cost. &
      Medium\,-- Decision $O((N+M)^{2})$; Training: High $O(E\!\cdot\!T)$ & Medium–High \\[2pt]

      Partition-Optimisation Dynamic Programming &
      Minimises the latest zone completion time in synchronised dynamic zone picking. &
      Saylam, Çelik \& Süral (2022) &
      DP enumerates adjacent zone boundaries to solve a min-max objective. &
      Balances workload and eases bottlenecks. &
      Computation grows quickly with more zones; limited real-time adaptability. &
      Medium -- $O(K\!\cdot\!Q)$ (pseudo-polynomial) & Medium–Low \\[2pt]

      Accelerated Storage Classification Algorithm &
      SKU class partitioning and space sharing based on turnover rate. &
      Rao \& Adil (2023) &
      Pruned dynamic programming that skips dominated partitions. &
      10--50 $\times$ faster than traditional DP while retaining optimality. &
      Relies on accurate demand forecasts; used as an off-line design tool. &
      Medium -- $O(K\!\cdot\!C)$ (pseudo-polynomial) & Low \\[2pt]

      Deep Q-Network (DQN) &
      Real-time multi-robot path planning and congestion management. &
      Alam, Khan \& Gunes (2024) &
      Grid-state DQN chooses the next action with rewards penalising collisions. &
      Millisecond-level decisions; greatly reduces deadlocks. &
      Requires large training data sets; sensitive to layout changes. &
      Training: High $O(E\!\cdot\!T)$; Inference: Low $O(1)$ & High \\[2pt]

      Federated Deep Reinforcement Learning &
      Privacy-preserving task scheduling across multiple warehouses. &
      Ho, Nguyen \& Cheriet (2024) &
      Federated PPO with local training and central model aggregation. &
      Keeps data local; supports heterogeneous warehouses; shares experience. &
      Communication overhead; slower convergence than centralised learning. &
      Training: High $O(E\!\cdot\!T\!\cdot\!A)+O(R\!\cdot\!B)$; Inference: Low $O(A)$ & High \\[2pt]

      Hierarchical Multi-Agent Reinforcement Learning &
      Task scheduling for multi-robot and human collaboration. &
      Krnjaic et al.\ (2023) &
      Manager–worker MARL with centralised training and distributed execution. &
      Adapts to varied layouts, delivers high pick rates, is sample-efficient. &
      High training compute needs; requires stable communication. &
      Training: High $O(E\!\cdot\!T\!\cdot\!A)$; Inference: Low $O(A)$ & High \\[2pt]
    \bottomrule
  \end{tabular}}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{IEEEtranN}
\bibliography{refs}
\end{document}
